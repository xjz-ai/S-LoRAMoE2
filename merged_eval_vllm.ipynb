{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for lora that has not route \n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import re\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import fire\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model_base = ''\n",
    "lora_model = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 记录gpu显存\n",
    "# import torch\n",
    "# torch.cuda.memory._record_memory_history()               # 开始记录\n",
    "# # run_your_code()                                          # 训练或推理代码\n",
    "# torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")   # 保存文件\n",
    "# torch.cuda.memory._record_memory_history(enabled=None)   # 终止记录\n",
    "# torch.cuda.list_gpu_processes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数\n",
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "    return\n",
    "\n",
    "def load_data(dataset) -> list:\n",
    "    \"\"\"\n",
    "    read data from dataset file\n",
    "    Args:\n",
    "        args:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    file_path = f'/home/xjz/proj/Subspace-Tuning/CR_MR/dataset/{dataset}/test.json'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"can not find dataset file : {file_path}\")\n",
    "    json_data = json.load(open(file_path, 'r'))\n",
    "    return json_data\n",
    "def extract_answer(dataset, sentence: str) -> float:\n",
    "    if dataset == 'boolq':\n",
    "        sentence_ = sentence.strip()\n",
    "        pred_answers = re.findall(r'true|false', sentence_)\n",
    "        if not pred_answers:\n",
    "            return \"\"\n",
    "        return pred_answers[0]\n",
    "    elif dataset == 'piqa':\n",
    "        sentence_ = sentence.strip()\n",
    "        pred_answers = re.findall(r'solution1|solution2', sentence_)\n",
    "        if not pred_answers:\n",
    "            return \"\"\n",
    "        return pred_answers[0]\n",
    "    elif dataset in ['social_i_qa', 'ARC-Challenge', 'ARC-Easy', 'openbookqa']:\n",
    "        sentence_ = sentence.strip()\n",
    "        pred_answers = re.findall(r'answer1|answer2|answer3|answer4|answer5', sentence_)\n",
    "        if not pred_answers:\n",
    "            return \"\"\n",
    "        return pred_answers[0]\n",
    "    elif dataset == 'hellaswag':\n",
    "        sentence_ = sentence.strip()\n",
    "        pred_answers = re.findall(r'ending1|ending2|ending3|ending4', sentence_)\n",
    "        if not pred_answers:\n",
    "            return \"\"\n",
    "        return pred_answers[0]\n",
    "    elif dataset == 'winogrande':\n",
    "        sentence_ = sentence.strip()\n",
    "        pred_answers = re.findall(r'option1|option2', sentence_)\n",
    "        if not pred_answers:\n",
    "            return \"\"\n",
    "        return pred_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载模型 \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_base)\n",
    "llm = LLM(\n",
    "        model = model_base,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.85,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\", \n",
    "        enforce_eager=False,\n",
    "        max_model_len=512,\n",
    "        enable_lora=True, #使用lora微调\n",
    "        max_lora_rank=32, #lora默认16，大的话要改\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义一个推理\n",
    "# 针对每个数据集 包装一个函数\n",
    "file_path = \"/home/xjz/proj/LLaMA-Factory/data/test_set.json\"\n",
    "datasets = json.load(open(file_path, 'r'))\n",
    "texts = []\n",
    "output_data = []\n",
    "correct = 0\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": datasets[i]['instruction']},\n",
    "    {\"role\": \"user\", \"content\": datasets[i]['input']},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    texts.append(text)\n",
    "# 推理\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8,top_k=10,repetition_penalty=1.05, max_tokens=4)\n",
    "outputs = llm.generate(texts, sampling_params,\n",
    "                    lora_request=LoRARequest(\"sui_bian_xie\", 1, lora_model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct  =0\n",
    "allnum = 0\n",
    "for i,output in enumerate(outputs):\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    # print(generated_text)\n",
    "    # if(len(generated_text) >1):\n",
    "        # allnum+=1\n",
    "    predict = generated_text[0]\n",
    "    label = datasets[i]['output']\n",
    "    if label == predict:\n",
    "        correct+=1\n",
    "    # else :\n",
    "        # print(\"predict\")\n",
    "        # print(generated_text)\n",
    "        # print(\"label\")\n",
    "        # print(label)\n",
    "print(correct/len(outputs))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllmO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
