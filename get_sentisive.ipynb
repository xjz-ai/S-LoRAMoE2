{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['boolq', 'piqa','social_i_qa','winogrande','ARC-Easy','ARC-Challenge','openbookqa','hellaswag']\n",
    "DATAPATH = r'dataset/'+dataset[0]+'/train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
    "# and proprietary rights in and to this software, related documentation\n",
    "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
    "# distribution of this software and related documentation without an express\n",
    "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import sys\n",
    "import json\n",
    "from typing import List\n",
    "from badam import BlockOptimizer\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "# from datasets import load_dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from typing import List, Optional, Union\n",
    "from transformers import TrainerState,TrainingArguments,TrainerControl\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "# 替换绝对路径即可\n",
    "sys.path.append(\"/home/xjz/proj/Subspace-Tuning/CR_MR/peft/src/\")\n",
    "from peft import (  # noqa: E402\n",
    "    LoraConfig,\n",
    "    DoraConfig,\n",
    "    BottleneckConfig,\n",
    "    PrefixTuningConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, AutoModel  # noqa: F402\n",
    "\n",
    "# model/data params\n",
    "base_model: str = \"/home/xjz/model/Qwen2_5_3B\"  # the only required argument\n",
    "data_path: str = DATAPATH\n",
    "output_dir: str = \"/data/xjz/test/\"+dataset[0]\n",
    "adapter_name: str = \"lora\"\n",
    "load_8bit : bool = False\n",
    "# training hyperparams\n",
    "batch_size:int = 1\n",
    "micro_batch_size:int =1\n",
    "num_epochs: int = 1.2\n",
    "learning_rate: float = 1e-5\n",
    "weight_decay: float = 0.0\n",
    "cutoff_len: int = 256\n",
    "val_set_size: int = 0\n",
    "use_gradient_checkpointing: bool = False\n",
    "eval_step: int = 0\n",
    "save_step: int = 3000\n",
    "# lora hyperparams\n",
    "lora_r: int = 8\n",
    "lora_alpha: int = 16\n",
    "lora_dropout: float = 0.05\n",
    "lora_target_modules: List[str] = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    "# bottleneck adapter hyperparams\n",
    "bottleneck_size: int = 256\n",
    "non_linearity: str = \"tanh\"\n",
    "adapter_dropout: float = 0.0\n",
    "use_parallel_adapter: bool = False\n",
    "use_adapterp: bool = False\n",
    "target_modules: List[str] = None\n",
    "# Dora hyperparams\n",
    "dora_simple: bool = True\n",
    "Wdecompose_target_modules: List[str] = None\n",
    "scaling: Union[float, str] = 1.0\n",
    "# prefix tuning hyperparams\n",
    "num_virtual_tokens: int = 30\n",
    "# llm hyperparams\n",
    "train_on_inputs: bool = True  # if False, masks out inputs in loss\n",
    "group_by_length: bool = False  # faster, but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project: str = \"\"\n",
    "wandb_run_name: str = \"\"\n",
    "wandb_watch: str = \"\" # options: false | gradients | all\n",
    "wandb_log_model: str = \"\"  # options: false | true\n",
    "resume_from_checkpoint: str = None  # either training checkpoint or final adapter\n",
    "\n",
    "gradient_accumulation_steps = int(batch_size) // int(micro_batch_size)\n",
    "\n",
    "device_map = \"auto\"\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps / world_size\n",
    "\n",
    "# Check if parameter passed or if set within environ\n",
    "use_wandb = len(wandb_project) > 0 or (\n",
    "    \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    ")\n",
    "# Only overwrite environ if wandb param passed\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "if len(wandb_watch) > 0:\n",
    "    os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "if len(wandb_log_model) > 0:\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
    "\n",
    "if load_8bit:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=load_8bit,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map={\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if model.config.model_type == \"llama\":\n",
    "# Due to the name of transformers' LlamaTokenizer, we have to do this\n",
    "# need to handle llama 3 separately\n",
    "    if \"Llama-3\" in base_model:\n",
    "        print(\"load llama-3 tokenizer\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    else:\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempgrad_dict = {}\n",
    "import pickle\n",
    "class SkipStepCallback(transformers.TrainerCallback):\n",
    "\n",
    "    grad_dict = {}\n",
    "    has_grad = 0\n",
    "    skip_kwd_list = ['input_layernorm', 'post_attention_layernorm', 'bias','lm_head','embed_tokens','rotary_emb','act_fn']\n",
    "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        return None\n",
    "       \n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def on_pre_optimizer_step(self, args, state, control, **kwargs):\n",
    "        \n",
    "        model = kwargs['model']\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if not any(kwd in name for kwd in self.skip_kwd_list):\n",
    "                    self.grad_dict[name] += (param.grad**2).sum().item()\n",
    "        self.has_grad = 0\n",
    "       \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        model =  kwargs['model']\n",
    "     \n",
    "        grad_skip_kwd_list = ['head', 'norm', 'bias','input_layernorm','post_attention_layernorm','input_layernorm', 'bias','lm_head','embed_tokens','rotary_emb','act_fn'] \n",
    "        for name, param in model.named_parameters():\n",
    "            if not any(kwd in name for kwd in grad_skip_kwd_list):\n",
    "                self.grad_dict[name] = 0.0\n",
    "                \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "    \n",
    "       \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        json_lines = []\n",
    "        for key, value in self.grad_dict.items():\n",
    "            json_line = json.dumps({key: value})\n",
    "            json_lines.append(json_line)\n",
    "        print(json_lines)\n",
    "        # 将每行 JSON 字符串写入文件\n",
    "        Toutput_dir = DATAPATH+r'expert3b1152.json'  \n",
    "        with open(Toutput_dir, 'w') as f:\n",
    "            for line in json_lines:\n",
    "                f.write(line + '\\n')\n",
    "        print(\"JSON file saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if adapter_name == \"prefix-tuning\":\n",
    "    model.to('cuda')\n",
    "\n",
    "if data_path.endswith(\".json\"):  # todo: support jsonl\n",
    "    data = load_dataset(\"json\", data_files=data_path)\n",
    "else:\n",
    "    data = load_dataset(data_path)\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "# Check the available weights and load them\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # Full checkpoint\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model - LoRA config above has to fit\n",
    "        resume_from_checkpoint = (\n",
    "            False  # So the trainer won't try loading its state\n",
    "        )\n",
    "    # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        model = set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "        \n",
    "# 修改为qwen2 start\n",
    "def process_func(example):\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{example['instruction'] + example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id] \n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1 \n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > cutoff_len:\n",
    "        input_ids = input_ids[:cutoff_len]   \n",
    "        attention_mask = attention_mask[:cutoff_len]    \n",
    "        labels = labels[:cutoff_len] \n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "if val_set_size > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=val_set_size, shuffle=True, seed=42\n",
    "    )\n",
    "    train_data = (\n",
    "        train_val[\"train\"].shuffle().map(process_func,remove_columns=data['train'].column_names)\n",
    "    )\n",
    "    val_data = (\n",
    "        train_val[\"test\"].shuffle().map(process_func,remove_columns=data['test'].column_names)\n",
    "    )\n",
    "else:\n",
    "    train_data = data['train'].shuffle().map(process_func,remove_columns=data['train'].column_names)\n",
    "    val_data = None\n",
    "# 修改为qwen2 end\n",
    "\n",
    "original_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler. ExponentialLR(original_optimizer, gamma=0.95)\n",
    "\n",
    "optimizer = BlockOptimizer(\n",
    "base_optimizer=original_optimizer, \n",
    "named_parameters_list=list(model.named_parameters()), \n",
    "switch_block_every=1, \n",
    "switch_mode=\"descending\",\n",
    "verbose=2 # information level, will print trainable parameters when setting to 2\n",
    ")\n",
    "trainer = transformers.Trainer(\n",
    "model=model,\n",
    "train_dataset=train_data,\n",
    "eval_dataset=val_data,\n",
    "\n",
    "args=transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    warmup_steps=20,\n",
    "    num_train_epochs=num_epochs,\n",
    "    max_steps = 108,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    # fp16=True,\n",
    "    logging_steps=20,\n",
    "    # 测试有用没\n",
    "    logging_dir= output_dir,\n",
    "    \n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "    eval_steps=eval_step if val_set_size > 0 else None,\n",
    "    \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_step,\n",
    "    output_dir=output_dir,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "    ddp_find_unused_parameters=False if ddp else None,\n",
    "    group_by_length=group_by_length,\n",
    "    report_to=\"wandb\" if use_wandb else None,\n",
    "    run_name=wandb_run_name if use_wandb else None,\n",
    "),\n",
    "data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "),\n",
    "optimizers=(optimizer, scheduler),\n",
    "\n",
    ")\n",
    "trainer.add_callback(SkipStepCallback)\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "metrics = train_result.metrics\n",
    "metrics[\"train_samples\"] = len(train_data)\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('grad_dict.pkl', 'rb') as f:\n",
    "#     loaded_grad_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# grad_skip_kwd_list = ['head', 'embed_tokens', 'norm', 'bias']  # Fully tune head and class token, freeze patch_embed,\n",
    "#                                                          # we find pos_embed can be either fully ft or unstructured ft, doesn't make much difference\n",
    "# grad_matrix_kwd_list = ['.q.', '.k.', '.v.', 'proj', 'fc']  # Might structurally tune the matrices: q, k, v, proj, fc1, and fc2\n",
    "# grad_vector_kwd_list = ['norm', 'bias']  # Might structurly tune the vectors\n",
    "# grad_dict = loaded_grad_dict\n",
    "# grad_shapes = {}\n",
    "# grad_shapes_int = {}\n",
    "# for key in grad_dict.keys():\n",
    "#         if not any(kwd in key for kwd in grad_skip_kwd_list):\n",
    "#             # print('yes')\n",
    "#             print(key)\n",
    "#             grad_tensor = grad_dict[key].to(device)\n",
    "#             sumvalue = grad_tensor.sum().item()\n",
    "#             grad_shapes[key] = sumvalue\n",
    "# #             grad_shapes_int[key] = np.cumprod(list(grad_dict[key].shape))[-1]\n",
    "\n",
    "# # large_tensor = torch.cat([grad_dict[key].flatten() for key in grad_shapes.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grad_shapes\n",
    "# # # 假设 self.grad_dict 是一个字典，其中每个值都是一个浮点数\n",
    "# # top_20 = dict(sorted(grad_shapes.items(), key=lambda item: item[1], reverse=True)[:20])\n",
    "\n",
    "# # # top_20 现在包含前 20 个最大值的项\n",
    "# # print(top_20)\n",
    "# threshold = 0.5\n",
    "# filtered_items = {k: v for k, v in grad_shapes.items() if v > threshold}\n",
    "# print(f\"Items with values greater than {threshold}:\")\n",
    "# print(len(filtered_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in filtered_items:\n",
    "#     filtered_items[key] = 1\n",
    "# filtered_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# def infer_param_groups(param_names, include_embedding = False, include_lm_head = False):\n",
    "#         \"\"\"automatic inference of the parameter groups based on the parameter names.\n",
    "#         divide groups into:\n",
    "#             * embedding\n",
    "#             * transformer layers\n",
    "#             * lm_head and others\n",
    "#         \"\"\"\n",
    "        \n",
    "#         block_prefix_list = []\n",
    "#         lm_head_and_other_params = []\n",
    "#         embed_pattern = r'.*embed[^.]*\\.'\n",
    "#         layer_pattern = r'.*layers.[^.]*\\.'\n",
    "\n",
    "#         for name in param_names:\n",
    "#             print(name)\n",
    "#             if any(prefix[0] in name for prefix in block_prefix_list):\n",
    "#                 continue\n",
    "            \n",
    "#             if re.findall(layer_pattern, name):\n",
    "#                 block_prefix_list.append(re.findall(layer_pattern, name))\n",
    "#             elif re.findall(embed_pattern, name) and include_embedding:\n",
    "#                 block_prefix_list.append(re.findall(embed_pattern, name))\n",
    "#             else:\n",
    "#                 lm_head_and_other_params.append(name)\n",
    "        \n",
    "#         if include_lm_head:\n",
    "#             block_prefix_list.append(lm_head_and_other_params)\n",
    "        \n",
    "#         return block_prefix_list\n",
    "\n",
    "# test_list =  infer_param_groups([n for n,_ in list(model.named_parameters())])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n,_ in list(model.named_parameters()):\n",
    "#     if 'lora' in n:\n",
    "#         print(n)\n",
    "\n",
    "# a = [1,2,3,4]\n",
    "# a.end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
